# KV cache和prompt

---

## 1️⃣ 概念

| 名称                        | 概念                                     | 特点                              |
| ------------------------- | -------------------------------------- | ------------------------------- |
| **Prompt（提示词）**           | 用户在调用模型时输入的文本或指令                       | 一次性输入，包含问题、上下文或指令；模型只看到你提供的内容   |
| **KV Cache（Key/Value缓存）** | 模型内部保存历史 token 的 Key/Value 向量，用于自注意力计算 | 内部状态，记录历史 token 的表示，支持增量推理和多轮对话 |

---

## 2️⃣ 作用

| 名称           | 作用                                  | 使用场景                   |
| ------------ | ----------------------------------- | ---------------------- |
| **Prompt**   | 提供模型当前需要理解或生成的文本上下文                 | 普通生成、一次性问答、需要外部提供历史上下文 |
| **KV Cache** | 保持模型内部历史记忆，允许增量输入新 token 时仍可访问完整上下文 | 流式推理、多轮对话、节省重复计算       |

---

## 3️⃣ 区别与联系

| 维度   | Prompt              | KV Cache                |
| ---- | ------------------- | ----------------------- |
| 存放位置 | 模型外部，由用户传入          | 模型内部，由生成过程自动维护          |
| 是否增量 | 通常一次性传入             | 支持增量 token 输入           |
| 历史记忆 | 不自动保留，需要每次拼接        | 自动保留历史 token，支持连续对话     |
| 计算效率 | 每次生成都需重新处理全部 prompt | 只计算新增 token，重用历史 K/V 向量 |

---

## 4️⃣ 实例对比

**场景：连续对话**

```
用户: 你是谁？
```

* **Prompt 方式**：生成回答时，需要输入完整 prompt `"用户: 你是谁？"`
* **KV Cache 方式**：生成回答时，只需输入 `"你是谁？"`，模型内部 KV cache 已经保存了之前的历史 token，上下文完整

---

## 5️⃣ 总结

* **Prompt** = 用户提供的输入，上下文信息由外部控制
* **KV Cache** = 模型内部记忆，保存历史 token 向量，支持流式生成和多轮对话
* **配合使用**：

  * 普通一次性生成 → 只用 prompt
  * 流式或多轮对话 → KV cache + 增量 prompt

---
